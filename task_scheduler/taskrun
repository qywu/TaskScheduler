#!/usr/bin/env python3
import os
import requests
import torch
import argparse
from omegaconf import OmegaConf


def tstring(astring):
    return astring


dirpath = os.path.dirname(os.path.realpath(__file__))
config = OmegaConf.load(os.path.join(dirpath, "config.yaml"))

parser = argparse.ArgumentParser()
# subparsers = parser.add_subparsers(help='for dealing with the command')

parser.add_argument("-g", "--gpus", type=int, default=0, help="number of gpus")

parser.add_argument(
    "-t",
    "--time_interval",
    type=int,
    default=0,
    help="the time interval (s) to start running the next task",
)

parser.add_argument(
    "-m", "--min_gpu_memory", type=int, default=-1, help="required gpu memory (MB)"
)

parser.add_argument(
    "--max_num_retries", type=int, default=-1, help="maximum number of retries"
)


args, rest = parser.parse_known_args()
command = " ".join(rest)

if args.gpus > 0:
    if not torch.cuda.is_available():
        print("CUDA is not available")
        exit(1)

    if args.min_gpu_memory == -1:
        # set the minimum gpu memory to the GPU memory, assume that the user wants to use the whole GPU - 5GB
        args.min_gpu_memory = (
            torch.cuda.get_device_properties(0).total_memory // 1024 // 1024 - 5000
        )

url = f"http://127.0.0.1:{config.server.port}/update_process"
myobj = {
    "path": os.getcwd(),
    "command": command,
    "gpus": args.gpus,
    "time_interval": args.time_interval,
    "min_gpu_memory": args.min_gpu_memory,
    "max_num_retries": args.max_num_retries,
    "password": config.user.admin.password,
}
x = requests.post(url, data=myobj)

print(x.text)
